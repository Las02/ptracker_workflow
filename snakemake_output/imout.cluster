Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=1000, mem_mib=954, disk_mb=1000, disk_mib=954, walltime=0, mem_gb=1
Select jobs to execute...

[Fri May 31 12:25:43 2024]
rule test_conda:
    output: test.delme
    log: /maps/projects/rasmussen/scratch/ptracker/ptracker/snakemake_output/imout, /maps/projects/rasmussen/scratch/ptracker/ptracker/snakemake_output/imout
    jobid: 0
    reason: Forced execution
    resources: mem_mb=1000, mem_mib=954, disk_mb=1000, disk_mib=954, tmpdir=/tmp, walltime=0, mem_gb=1


            echo "HELLO"
            sleep 10
            snakemake &> test.delme
            
/usr/bin/bash: activate: No such file or directory
HELLO
[Fri May 31 12:25:54 2024]
Error in rule test_conda:
    jobid: 0
    output: test.delme
    log: /maps/projects/rasmussen/scratch/ptracker/ptracker/snakemake_output/imout, /maps/projects/rasmussen/scratch/ptracker/ptracker/snakemake_output/imout (check log file(s) for error details)
    shell:
        
            echo "HELLO"
            sleep 10
            snakemake &> test.delme
            
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job test_conda since they might be corrupted:
test.delme
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
SNAKEDIR bin/ptracker/src/workflow
